"use strict";(self.webpackChunkgo_micro=self.webpackChunkgo_micro||[]).push([[9318],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>g});var o=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=o.createContext({}),c=function(e){var t=o.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},d=function(e){var t=c(e.components);return o.createElement(s.Provider,{value:t},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},u=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),p=c(n),u=a,g=p["".concat(s,".").concat(u)]||p[u]||m[u]||i;return n?o.createElement(g,r(r({ref:t},d),{},{components:n})):o.createElement(g,r({ref:t},d))}));function g(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,r=new Array(i);r[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[p]="string"==typeof e?e:a,r[1]=l;for(var c=2;c<i;c++)r[c]=n[c];return o.createElement.apply(null,r)}return o.createElement.apply(null,n)}u.displayName="MDXCreateElement"},6389:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var o=n(7462),a=(n(7294),n(3905));const i={title:"Go-Micro easily connects to the EFK log system",description:"Golang microservice framework (Go-Micro) easily connects to the EFK log system",tags:["go","golang","microservice","go-micro","log","elasticsearch","fluentd","kibana"],hide_table_of_contents:!1},r=void 0,l={permalink:"/2023/09/06/go-micro-easily-connects-to-efk-log-system",source:"@site/blog/2023-09-06-go-micro-easily-connects-to-efk-log-system.md",title:"Go-Micro easily connects to the EFK log system",description:"Golang microservice framework (Go-Micro) easily connects to the EFK log system",date:"2023-09-06T00:00:00.000Z",formattedDate:"September 6, 2023",tags:[{label:"go",permalink:"/tags/go"},{label:"golang",permalink:"/tags/golang"},{label:"microservice",permalink:"/tags/microservice"},{label:"go-micro",permalink:"/tags/go-micro"},{label:"log",permalink:"/tags/log"},{label:"elasticsearch",permalink:"/tags/elasticsearch"},{label:"fluentd",permalink:"/tags/fluentd"},{label:"kibana",permalink:"/tags/kibana"}],readingTime:5.55,hasTruncateMarker:!0,authors:[],frontMatter:{title:"Go-Micro easily connects to the EFK log system",description:"Golang microservice framework (Go-Micro) easily connects to the EFK log system",tags:["go","golang","microservice","go-micro","log","elasticsearch","fluentd","kibana"],hide_table_of_contents:!1},nextItem:{title:"Golang microservice framework (Go-Micro) implements SSE service",permalink:"/2023/08/24/go-micro-implements-sse-service"}},s={authorsImageUrls:[]},c=[{value:"Overview",id:"overview",level:2},{value:"EFK - Distributed Log System",id:"efk---distributed-log-system",level:2},{value:"Deploy EFK",id:"deploy-efk",level:2},{value:"Go-Micro interface with EFK",id:"go-micro-interface-with-efk",level:2},{value:"Kibana query log",id:"kibana-query-log",level:2},{value:"References",id:"references",level:2}],d={toc:c},p="wrapper";function m(e){let{components:t,...n}=e;return(0,a.kt)(p,(0,o.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"overview"},"Overview"),(0,a.kt)("p",null,"In the early era of single services, if we wanted to use logs to locate business logic bugs or performance issues in the production environment, we have to operation and maintenance personnel log in to the server remotely one by one and query the log files for each service instance to troubleshoot the problem."),(0,a.kt)("p",null,"In the era of microservices, service instances are deployed on different physical machines, and the logs of each microservice are also scattered and stored on different machines.\nWhen the service cluster is large enough, hundreds, thousands, or even tens of thousands, it is already an impossible task to use the above traditional method to check the log.\nTherefore, we need to centralize the management of logs in the distributed system."),(0,a.kt)("p",null,"Log collection is an integral part of microservice observability.\nLogs are useful for debugging problems and monitoring cluster health.\nHowever, after collecting log files centrally, is still not all right. We have a series of problems to solve like: "),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"How to query and analyze log files."),(0,a.kt)("li",{parentName:"ul"},"Which services have alarms and exceptions also require detailed statistics.")),(0,a.kt)("p",null,"Therefore, when online failures occurred in the past, it was often seen that development, operation, and maintenance personnel downloaded service logs, and retrieved, and counted them based on some commands under Linux (such as ",(0,a.kt)("inlineCode",{parentName:"p"},"grep"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"awk"),", and ",(0,a.kt)("inlineCode",{parentName:"p"},"wc"),", etc.)\nThis old method not only has a heavy workload, and is inefficient, but also cannot handle more demanding operations such as query, sorting, and statistics, as well as a large number of cluster machines."),(0,a.kt)("p",null,"EFK (Elasticsearch, Fluentd, Kibana) is a popular distributed log service solution in current microservices.\nBelow, I will explain step by step how EFK is applied to the Golang microservice framework (Go-Micro)."),(0,a.kt)("h2",{id:"efk---distributed-log-system"},"EFK - Distributed Log System"),(0,a.kt)("p",null,"EFK is a complete-distributed log collection system, which well solves the above-mentioned problems of difficulty in log collection, retrieval, and analysis."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"Elasticsearch")," is a distributed search engine.\nIt has the characteristics of high scalability, high reliability, and easy management.\nIt can be used for full-text search, structured search, and analysis, and can combine the three.\nElasticsearch is developed based on ",(0,a.kt)("strong",{parentName:"li"},"Lucene")," and is now one of the most widely used open-source search engines.\nWikipedia, StackOverflow, Github, etc. all build their own search engines based on it."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"Fluentd")," is an open-source data collector.\nWe can install Fluentd on the microservice cluster node to obtain container log files, filter and transform the log data and then pass the data to the Elasticsearch cluster, where it will be indexed and stored.\nFluentd's logs must be in JSON as the carrier, and there is no limit to the log content."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"Kibana")," is a visualization platform.\nKibana is a web page used to search, analyze, and visualize log data stored in Elasticsearch metrics.\nKibana leverages Elasticsearch's interface to retrieve data, call data stored in Elasticsearch, and visualize it.\nIt not only allows users to customize views, but also supports querying and filtering data in special ways.")),(0,a.kt)("p",null,"If we regard this series of components as an MVC model, then it is:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"Fluentd")," corresponds to the logic control (",(0,a.kt)("strong",{parentName:"li"},"Controller layer"),")"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"Elasticsearch")," is a data model (",(0,a.kt)("strong",{parentName:"li"},"Model layer"),")"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"Kibana")," is a view (",(0,a.kt)("strong",{parentName:"li"},"View layer"),")")),(0,a.kt)("h2",{id:"deploy-efk"},"Deploy EFK"),(0,a.kt)("p",null,"We use Docker to deploy EFK.\nFirst, let's write a Docker-Compose configuration file:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'version: \'3\'\n\nnetworks:\n  go-micro-efk:\n    driver: bridge\n\nservices:\n  elasticsearch:\n    image: docker.io/bitnami/elasticsearch:latest\n    networks:\n      - go-micro-efk\n    ports:\n      - "9200:9200"\n      - "9300:9300"\n    environment:\n      - ELASTICSEARCH_USERNAME=elastic\n      - ELASTICSEARCH_PASSWORD=elastic\n      - xpack.security.enabled=true\n      - discovery.type=single-node\n      - http.cors.enabled=true\n      - http.cors.allow-origin=http://localhost:13580,http://127.0.0.1:13580\n      - http.cors.allow-headers=X-Requested-With,X-Auth-Token,Content-Type,Content-Length,Authorization\n      - http.cors.allow-credentials=true\n\n  fluentd:\n    image: docker.io/bitnami/fluentd:latest\n    networks:\n      - go-micro-efk\n    depends_on:\n      - "elasticsearch"\n    volumes:\n      - ./fluentd/conf:/opt/bitnami/fluentd/conf\n      - ./fluentd/log:/opt/bitnami/fluentd/log\n    ports:\n      - "24224:24224"\n      - "24224:24224/udp"\n\n  kibana:\n    image: docker.io/bitnami/kibana:latest\n    networks:\n      - go-micro-efk\n    depends_on:\n      - "elasticsearch"\n    ports:\n      - "5601:5601"\n    environment:\n      - KIBANA_ELASTICSEARCH_URL=elasticsearch\n      - KIBANA_ELASTICSEARCH_PORT_NUMBER=9200\n')),(0,a.kt)("p",null,"Then, we use the following command to create a Docker container and run it in the background:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"docker-compose up -d\n")),(0,a.kt)("p",null,"This is not the end, we still need to modify the configuration of Fluentd.\nIn the original configuration, the log is only recorded in the local text file of Fluentd.\nSo we need to modify the configuration to enable logging to Elasticsearch."),(0,a.kt)("p",null,"The original configuration file looks like this:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml"},"<source>\n  @type  forward\n  @id    input1\n  @label @mainstream\n  port  24224\n</source>\n\n<filter **>\n  @type stdout\n</filter>\n\n<label @mainstream>\n  <match docker.**>\n    @type file\n    @id             output_docker1\n    path            /opt/bitnami/fluentd/logs/docker.*.log\n    symlink_path    /opt/bitnami/fluentd/logs/docker.log\n    append          true\n    time_slice_format %Y%m%d\n    time_slice_wait   1m\n    time_format       %Y%m%dT%H%M%S%z\n  </match>\n  <match **>\n    @type file\n    @id   output1\n    path            /opt/bitnami/fluentd/logs/data.*.log\n    symlink_path    /opt/bitnami/fluentd/logs/data.log\n    append          true\n    time_slice_format %Y%m%d\n    time_slice_wait   10m\n    time_format       %Y%m%dT%H%M%S%z\n  </match>\n</label>\n\n# Include config files in the ./config.d directory\n@include config.d/*.conf\n")),(0,a.kt)("p",null,"We need to modify the ",(0,a.kt)("inlineCode",{parentName:"p"},"<match **>")," node to the following configuration:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-xml"},"<match **>\n    @type elasticsearch\n    host host.docker.internal\n    port 9200\n    index_name go-micro-fluentd\n    type_name log\n</match>\n")),(0,a.kt)("p",null,"The above configuration actually uses the ",(0,a.kt)("inlineCode",{parentName:"p"},"fluent-plugin-elasticsearch")," plug-in to import logs into Elasticsearch."),(0,a.kt)("h2",{id:"go-micro-interface-with-efk"},"Go-Micro interface with EFK"),(0,a.kt)("p",null,"First, we need to download a package of Fluent:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"go get github.com/devexps/go-micro/log/fluent/v2\n")),(0,a.kt)("p",null,"Then create the logger:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},'import (\n    fluentLogger "github.com/devexps/go-micro/log/fluent/v2"\n)\n\n// NewFluentLogger creates a new logger - Fluent\nfunc NewFluentLogger(endpoint string) log.Logger {\n    wrapped, err := fluentLogger.NewLogger(endpoint)\n    if err != nil {\n        panic("create fluent logger failed")\n        return nil\n    }\n    return wrapped\n}\n')),(0,a.kt)("p",null,"Now, all Go-Micro logs are injected into the EFK."),(0,a.kt)("h2",{id:"kibana-query-log"},"Kibana query log"),(0,a.kt)("p",null,"Kibana's access port is 5601, so we can access: ",(0,a.kt)("a",{parentName:"p",href:"http://127.0.0.1:5601/"},"http://127.0.0.1:5601/")),(0,a.kt)("p",null,"When we first entered Kibana, it was blank and nothing. "),(0,a.kt)("p",{align:"center"},(0,a.kt)("img",{src:"/assets/thoughts/go-micro/connect-to-efk-log-system/screenshot-001.png",width:"100%",alt:"go-micro-connects-to-the-efk-log-system-screenshot-001"})),(0,a.kt)("p",null,"Then, we need to add a ",(0,a.kt)("inlineCode",{parentName:"p"},"Data View"),", which is equivalent to creating a log query view.\nWe click ",(0,a.kt)("inlineCode",{parentName:"p"},"Discover")," -> ",(0,a.kt)("inlineCode",{parentName:"p"},"Create a data view"),". After that, we will see the following interface:"),(0,a.kt)("p",{align:"center"},(0,a.kt)("img",{src:"/assets/thoughts/go-micro/connect-to-efk-log-system/screenshot-002.png",width:"100%",alt:"go-micro-connects-to-the-efk-log-system-screenshot-002"})),(0,a.kt)("p",null,"Above, we set the elastic search index in fluent to: ",(0,a.kt)("inlineCode",{parentName:"p"},"go-micro-fluentd"),", so now we fill in ",(0,a.kt)("inlineCode",{parentName:"p"},"go-micro-fluentd")," in the ",(0,a.kt)("inlineCode",{parentName:"p"},"Index pattern")," text box.\nWith the ",(0,a.kt)("inlineCode",{parentName:"p"},"Timestamp field"),", we can find ",(0,a.kt)("inlineCode",{parentName:"p"},"ts")," from the drop-down box and select it.\nThen, we can click ",(0,a.kt)("inlineCode",{parentName:"p"},"Save data view to Kibana")," to create the view."),(0,a.kt)("p",null,"We will see a view of the log query:"),(0,a.kt)("p",{align:"center"},(0,a.kt)("img",{src:"/assets/thoughts/go-micro/connect-to-efk-log-system/screenshot-003.png",width:"100%",alt:"go-micro-connects-to-the-efk-log-system-screenshot-003"})),(0,a.kt)("p",null,"In this view, what we see is only the most original log information.\nWe can check ",(0,a.kt)("inlineCode",{parentName:"p"},"msg")," or other fields that need attention on the left, and Kibana will filter out the information we care about:"),(0,a.kt)("p",{align:"center"},(0,a.kt)("img",{src:"/assets/thoughts/go-micro/connect-to-efk-log-system/screenshot-004.png",width:"100%",alt:"go-micro-connects-to-the-efk-log-system-screenshot-004"})),(0,a.kt)("p",null,"If you need to build a more complex query, you can build a query statement in the top search bar."),(0,a.kt)("p",null,"Code repository: ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/devexps/go-efk"},"https://github.com/devexps/go-efk")),(0,a.kt)("h2",{id:"references"},"References"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://x-team.com/blog/improve-your-logging-process/"},"Improve your logging process")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://logz.io/blog/fluentd-Logstash/"},"Fluentd vs Logstash: A Comparison of Log Collectors")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://www.digitalocean.com/community/tutorials/how-to-set-up-an-elasticsearch-fluentd-and-kibana-efk-logging-stack-on-kubernetes"},"How to setup the LFK logging stack on Kubernetes"))))}m.isMDXComponent=!0}}]);